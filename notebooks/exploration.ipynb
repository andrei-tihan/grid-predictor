{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# src/data_prep.py\n",
    "\n",
    "# Script to load, clean, and merge Alberta generation and load CSVs.\n",
    "# Outputs a cleaned CSV with consistent hourly timestamps and engineered features.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TZ = \"America/Edmonton\"\n",
    "\n",
    "def _normalize_col(c):\n",
    "    if not isinstance(c, str):\n",
    "        return c\n",
    "    s = c.strip().lower()\n",
    "    s = s.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    s = s.replace(\"/\", \"_\").replace(\"-\", \"_\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "    s = \"_\".join(s.split())\n",
    "    return s\n",
    "\n",
    "def _find_by_keywords(cols, keywords_list):\n",
    "    \"\"\"\n",
    "    Return first column name that contains all keywords in one of the keyword lists.\n",
    "    keywords_list: list of keyword-lists (priority)\n",
    "    \"\"\"\n",
    "    norm = {c: _normalize_col(c) for c in cols}\n",
    "    for keywords in keywords_list:\n",
    "        for c, nc in norm.items():\n",
    "            if all(k in nc for k in keywords):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def _to_numeric(s):\n",
    "    return pd.to_numeric(s.astype(str).str.replace(\",\",\"\").str.strip(), errors=\"coerce\")\n",
    "\n",
    "def load_generation_csv(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # find date col\n",
    "    date_col = _find_by_keywords(cols, [[\"date\"], [\"time\"], [\"date_time\"], [\"date_-_mst\"], [\"date_mst\"]]) or cols[0]\n",
    "\n",
    "    # find main columns\n",
    "    total_gen_col = _find_by_keywords(cols, [[\"total\",\"generation\"], [\"total_generation\"], [\"total_gen\"], [\"total\"]])\n",
    "    wind_col = _find_by_keywords(cols, [[\"wind\"]])\n",
    "    solar_col = _find_by_keywords(cols, [[\"solar\"]])\n",
    "    other_col = _find_by_keywords(cols, [[\"other\"]])\n",
    "\n",
    "    # Build df with only relevant columns if present\n",
    "    keep = [date_col]\n",
    "    for c in (total_gen_col, wind_col, solar_col, other_col):\n",
    "        if c and c not in keep:\n",
    "            keep.append(c)\n",
    "    df2 = df[keep].copy()\n",
    "\n",
    "    # parse timestamp\n",
    "    df2['timestamp'] = pd.to_datetime(df2[date_col], format=\"%m/%d/%Y %I:%M:%S %p\", errors='coerce')\n",
    "    # localize to TZ (strings appear to be local MST)\n",
    "    if df2['timestamp'].dt.tz is None:\n",
    "        try:\n",
    "            df2['timestamp'] = df2['timestamp'].dt.tz_localize(TZ, ambiguous='infer', nonexistent='shift_forward')\n",
    "        except Exception:\n",
    "            df2['timestamp'] = df2['timestamp'].dt.tz_localize(TZ, ambiguous='NaT', nonexistent='NaT')\n",
    "    else:\n",
    "        df2['timestamp'] = df2['timestamp'].dt.tz_convert(TZ)\n",
    "\n",
    "    # numeric fields\n",
    "    df2['total_generation_mw'] = _to_numeric(df2[total_gen_col]) if total_gen_col else np.nan\n",
    "    df2['wind_mw'] = _to_numeric(df2[wind_col]) if wind_col else np.nan\n",
    "    df2['solar_mw'] = _to_numeric(df2[solar_col]) if solar_col else np.nan\n",
    "    if other_col:\n",
    "        df2['other_orig_mw'] = _to_numeric(df2[other_col])\n",
    "    else:\n",
    "        df2['other_orig_mw'] = np.nan\n",
    "\n",
    "    out = df2[['timestamp','total_generation_mw','wind_mw','solar_mw','other_orig_mw']].copy()\n",
    "    return out\n",
    "\n",
    "def load_load_csv(path: Path):\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    date_col = _find_by_keywords(cols, [[\"date\"], [\"time\"], [\"date_time\"], [\"date_-_mst\"], [\"date_mst\"]]) or cols[0]\n",
    "    load_col = _find_by_keywords(cols, [[\"ail\"], [\"load\"], [\"demand\"]])\n",
    "    if load_col is None:\n",
    "        raise ValueError(f\"Could not find AIL/load column in {path}. Columns: {cols}\")\n",
    "    df2 = df[[date_col, load_col]].copy()\n",
    "    df2['timestamp'] = pd.to_datetime(df2[date_col], errors='coerce')\n",
    "    if df2['timestamp'].dt.tz is None:\n",
    "        try:\n",
    "            df2['timestamp'] = df2['timestamp'].dt.tz_localize(TZ, ambiguous='infer', nonexistent='shift_forward')\n",
    "        except Exception:\n",
    "            df2['timestamp'] = df2['timestamp'].dt.tz_localize(TZ, ambiguous='NaT', nonexistent='NaT')\n",
    "    else:\n",
    "        df2['timestamp'] = df2['timestamp'].dt.tz_convert(TZ)\n",
    "    df2['load_mw'] = _to_numeric(df2[load_col])\n",
    "    return df2[['timestamp','load_mw']]\n",
    "\n",
    "def merge_and_clean(gen_df, load_df, out_path: Path):\n",
    "    # Outer merge to preserve any timestamps that might be missing in one\n",
    "    df = pd.merge(gen_df, load_df, on='timestamp', how='outer', sort=True)\n",
    "\n",
    "    # Ensure datetime sorted, dedupe\n",
    "    df = df.sort_values('timestamp').drop_duplicates('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Compute other_gen (if wind/solar present)\n",
    "    if 'wind_mw' in df.columns and 'solar_mw' in df.columns:\n",
    "        df['wind_mw'] = df['wind_mw'].fillna(0.0)\n",
    "        df['solar_mw'] = df['solar_mw'].fillna(0.0)\n",
    "        df['other_gen_mw'] = df['total_generation_mw'] - (df['wind_mw'] + df['solar_mw'])\n",
    "    else:\n",
    "        df['other_gen_mw'] = df.get('other_orig_mw', np.nan)\n",
    "\n",
    "    # Compute balance\n",
    "    df['balance_mw'] = df['total_generation_mw'] - df['load_mw']\n",
    "\n",
    "    # Check continuity and fill small gaps\n",
    "    ts_min = df['timestamp'].min()\n",
    "    ts_max = df['timestamp'].max()\n",
    "    expected_idx = pd.date_range(ts_min, ts_max, freq='h', tz=TZ)\n",
    "    if len(expected_idx) != df.shape[0]:\n",
    "        # reindex & interpolate small gaps\n",
    "        df = df.set_index('timestamp').reindex(expected_idx)\n",
    "        df.index.name = 'timestamp'\n",
    "        numeric_cols = ['total_generation_mw','load_mw','wind_mw','solar_mw','other_gen_mw','balance_mw']\n",
    "        for c in numeric_cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = df[c].interpolate(limit=2)\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Print basic sanity\n",
    "    missing_gen = df['total_generation_mw'].isna().sum()\n",
    "    missing_load = df['load_mw'].isna().sum()\n",
    "    print(\"Sanity checks after merge:\")\n",
    "    print(f\"  rows: {len(df)}\")\n",
    "    print(f\"  missing generation rows: {missing_gen}\")\n",
    "    print(f\"  missing load rows: {missing_load}\")\n",
    "\n",
    "    # Feature engineering (basic)\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['dow'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
    "\n",
    "    for lag in [1,24,168]:\n",
    "        df[f'balance_lag{lag}'] = df['balance_mw'].shift(lag)\n",
    "    for w in [24,168]:\n",
    "        df[f'balance_roll_{w}'] = df['balance_mw'].rolling(w).mean().shift(1)\n",
    "\n",
    "    # Save a clean CSV\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote cleaned CSV to {out_path}\")\n",
    "    return df\n",
    "\n",
    "def load_and_prepare(gen_csv, load_csv, out_path=Path(\"data/processed/alberta_hourly_clean.csv\")):\n",
    "    gen = Path(gen_csv)\n",
    "    load = Path(load_csv)\n",
    "    if not gen.exists() or not load.exists():\n",
    "        raise FileNotFoundError(\"Generation / Load CSV not found at paths provided.\")\n",
    "    gen_df = load_generation_csv(gen)\n",
    "    load_df = load_load_csv(load)\n",
    "    return merge_and_clean(gen_df, load_df, out_path)\n",
    "\n"
   ],
   "id": "9445c2580554544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training script\n",
    "\n",
    "Builds regression, classification, and clustering models for Alberta grid data.\n",
    "Saves models to models/ and predictions to data/processed/predictions.csv for use in Streamlit app."
   ],
   "id": "f6c18d87f6d2534a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from . import data_prep, features, recommend, evaluate, utils\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def build_ann_regressor_sklearn():\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    return (\"sklearn_mlp\", MLPRegressor(hidden_layer_sizes=(64,32), max_iter=500, random_state=42))\n",
    "\n",
    "def build_ann_regressor_keras(input_dim):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adamW', loss='mae')\n",
    "    return (\"keras\", model)\n",
    "\n",
    "def main(args):\n",
    "    cfg = utils.load_yaml(args.config) if args.config else {\n",
    "        \"tz\":\"America/Edmonton\",\"test_days\":90,\"lags\":[1,24,168],\"rolls\":[24,168],\n",
    "        \"neutral_k\":0.25,\"high_k\":0.75,\"models\":{\"regressor\":\"hgb\",\"classifier\":\"rf\"},\"n_clusters_daily\":3\n",
    "    }\n",
    "\n",
    "    # Directories\n",
    "    Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Data loading and preparation (produces data/processed/alberta_hourly_clean.csv)\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    df_clean = data_prep.load_and_prepare(args.gen, args.load, Path(\"data/processed/alberta_hourly_clean.csv\"))\n",
    "\n",
    "    # Features\n",
    "    df_feat = features.make_features(df_clean, lags=tuple(cfg.get(\"lags\",[1,24,168])), rolls=tuple(cfg.get(\"rolls\",[24,168])))\n",
    "    feats = features.feature_list(df_feat)\n",
    "    target = 'balance_mw'\n",
    "\n",
    "    # Train/test split (time-based)\n",
    "    last_ts = df_feat['timestamp'].max()\n",
    "    cutoff = last_ts - pd.Timedelta(days=cfg.get(\"test_days\",90))\n",
    "    train_df = df_feat[df_feat['timestamp'] <= cutoff].reset_index(drop=True)\n",
    "    test_df  = df_feat[df_feat['timestamp'] > cutoff].reset_index(drop=True)\n",
    "    print(\"Train rows:\", len(train_df), \"Test rows:\", len(test_df))\n",
    "\n",
    "    Xtr = train_df[feats]\n",
    "    ytr = train_df[target]\n",
    "    Xte = test_df[feats]\n",
    "    yte = test_df[target]\n",
    "\n",
    "    sigma_train = float(ytr.std())\n",
    "\n",
    "    # Regression model selection\n",
    "    reg_choice = cfg.get(\"models\",{}).get(\"regressor\",\"ann\")\n",
    "    print(\"Training regressor:\", reg_choice)\n",
    "    if reg_choice == \"hgb\":\n",
    "        reg = HistGradientBoostingRegressor(random_state=42)\n",
    "        reg.fit(Xtr, ytr)\n",
    "        ypred = reg.predict(Xte)\n",
    "        joblib.dump(reg, \"models/regressor.joblib\")\n",
    "    elif reg_choice == \"ridge\":\n",
    "        reg = Ridge(random_state=42)\n",
    "        reg.fit(Xtr, ytr)\n",
    "        ypred = reg.predict(Xte)\n",
    "        joblib.dump(reg, \"models/regressor.joblib\")\n",
    "    elif reg_choice == \"ann\":\n",
    "        backend, model = build_ann_regressor_keras(Xtr.shape[1])\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xte_s = scaler.transform(Xte)\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',  # Which metric to monitor\n",
    "            patience=10,  # Stop after 10 epochs with no improvement\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        model.fit(Xtr_s, ytr, validation_split=0.1, epochs=100, batch_size=32, verbose=2, callbacks=[early_stop])\n",
    "        ypred = model.predict(Xte_s).ravel()\n",
    "        model.save(\"models/regressor.keras\")\n",
    "        joblib.dump(scaler, \"models/scaler_ann.joblib\")\n",
    "    else:\n",
    "        # default to hgb\n",
    "        reg = HistGradientBoostingRegressor(random_state=42)\n",
    "        reg.fit(Xtr, ytr)\n",
    "        ypred = reg.predict(Xte)\n",
    "        joblib.dump(reg, \"models/regressor.joblib\")\n",
    "\n",
    "    reg_metrics = evaluate.regression_metrics(yte, ypred)\n",
    "    print(\"Regression metrics:\", reg_metrics)\n",
    "\n",
    "    # compute resid std on training (for band)\n",
    "    try:\n",
    "        if reg_choice == \"ann\":\n",
    "            scaler = joblib.load(\"models/scaler_ann.joblib\")\n",
    "            train_pred = model.predict(scaler.transform(Xtr)).ravel()\n",
    "        else:\n",
    "            train_pred = reg.predict(Xtr)\n",
    "        resid_std = float((ytr - train_pred).std())\n",
    "    except Exception:\n",
    "        resid_std = float((ytr - reg.predict(Xtr)).std())\n",
    "\n",
    "    # Step 5: classification (3-class)\n",
    "    neutral_k = cfg.get(\"neutral_k\", 0.25)\n",
    "    high_k = cfg.get(\"high_k\", 0.75)\n",
    "    def label_from_balance(x):\n",
    "        if x >= high_k * sigma_train: return 2\n",
    "        if x <= -high_k * sigma_train: return 0\n",
    "        if abs(x) <= neutral_k * sigma_train: return 1\n",
    "        return 2 if x > 0 else 0\n",
    "\n",
    "    df_feat['label'] = df_feat['balance_mw'].apply(label_from_balance)\n",
    "    train_df = df_feat[df_feat['timestamp'] <= cutoff].reset_index(drop=True)\n",
    "    test_df  = df_feat[df_feat['timestamp'] > cutoff].reset_index(drop=True)\n",
    "    clf_feats = feats\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(train_df[clf_feats], train_df['label'])\n",
    "    ypred_clf = clf.predict(test_df[clf_feats])\n",
    "    clf_metrics = evaluate.classification_metrics(test_df['label'], ypred_clf)\n",
    "    print(\"Classifier metrics:\", clf_metrics)\n",
    "    joblib.dump(clf, \"models/classifier.joblib\")\n",
    "\n",
    "    # Step 6: clustering daily profiles\n",
    "    daily = df_clean[['timestamp','balance_mw']].copy()\n",
    "    daily['date'] = daily['timestamp'].dt.date\n",
    "    daily['hour'] = daily['timestamp'].dt.hour\n",
    "    daily_pivot = daily.pivot_table(\n",
    "        index='date',\n",
    "        columns='hour',\n",
    "        values='balance_mw',\n",
    "        aggfunc='mean'\n",
    "    ).fillna(method='ffill', axis=1)\n",
    "    sc = StandardScaler()\n",
    "    X_daily = sc.fit_transform(daily_pivot.fillna(0))\n",
    "    n_clusters = cfg.get(\"n_clusters_daily\", 3)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    daily_labels = kmeans.fit_predict(X_daily)\n",
    "    sil = silhouette_score(X_daily, daily_labels)\n",
    "    joblib.dump(kmeans, \"models/kmeans_daily.joblib\")\n",
    "\n",
    "    # Build predictions.csv for app\n",
    "    out = test_df[['timestamp','balance_mw']].copy()\n",
    "    out['y_pred'] = ypred\n",
    "    out['y_lower'] = out['y_pred'] - resid_std\n",
    "    out['y_upper'] = out['y_pred'] + resid_std\n",
    "    out['band'] = out['y_pred'].apply(lambda x: recommend.band_label(x, sigma_train, neutral_k=neutral_k, high_k=high_k))\n",
    "    out['grid_description'] = out['band'].apply(lambda b: recommend.recommendation_from_band(b)['description'])\n",
    "    out['grid_recommendation'] = out['band'].apply(lambda b: recommend.recommendation_from_band(b)['grid_action'])\n",
    "    out['customer_recommendation'] = out['band'].apply(lambda b: recommend.recommendation_from_band(b)['customer_action'])\n",
    "    out.to_csv(\"data/processed/predictions.csv\", index=False)\n",
    "    print(\"Wrote predictions.csv (for Streamlit app)\")\n",
    "\n",
    "    # Step 8: save meta\n",
    "    meta = {\n",
    "        \"regression_metrics\": reg_metrics,\n",
    "        \"classification_metrics\": clf_metrics,\n",
    "        \"clustering_silhouette\": float(sil),\n",
    "        \"sigma_train\": float(sigma_train),\n",
    "        \"resid_std\": float(resid_std),\n",
    "        \"features\": feats\n",
    "    }\n",
    "    utils.save_json(\"models/meta.json\", meta)\n",
    "    print(\"Saved models/meta.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gen\", required=True, help=\"path to generation CSV\")\n",
    "    parser.add_argument(\"--load\", required=True, help=\"path to load CSV\")\n",
    "    parser.add_argument(\"--config\", default=\"config.yaml\", help=\"path to config yaml\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ],
   "id": "491c93bc7e95b7dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
